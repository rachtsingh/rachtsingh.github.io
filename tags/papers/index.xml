<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Papers on Rachit Singh</title>
    <link>https://rachitsingh.com/tags/papers/</link>
    <description>Recent content in Papers on Rachit Singh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 23 Dec 2017 12:08:00 -0800</lastBuildDate>
    
	<atom:link href="https://rachitsingh.com/tags/papers/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ELBO Surgery</title>
      <link>https://rachitsingh.com/elbo_surgery/</link>
      <pubDate>Sat, 23 Dec 2017 12:08:00 -0800</pubDate>
      
      <guid>https://rachitsingh.com/elbo_surgery/</guid>
      <description>td { padding: 5px; font-family: monospace; font-size: 1.25rem; } th { text-align: center; padding: 0px 5px; } th.left_column { text-align: right; } figure { margin: 0px 20px; max-width: 50rem; } img[src*=&#34;#smaller&#34;] { width: 50%; margin: auto; }  tldr: The ubiquitous isotropic Gaussian prior for generative models doesn&#39;t make sense / doesn&#39;t work, which motivates work on priors.
At NIPS, Dawen Liang mentioned Hoffman &amp;amp; Johnson&#39;s ELBO surgery paper offhand while talking about tuning KL divergences, and it&#39;s very interesting, so I thought I&#39;d go over it.</description>
    </item>
    
  </channel>
</rss>