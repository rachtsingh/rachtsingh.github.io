---
title: "PyTorch Internals, cuRAND, and numerical instability"
date: 2018-01-03T18:44:27-08:00
draft: false
---

## Random sampling

I've been working lately to implement random samplers from a number of distributions in PyTorch, both on CPU and CUDA. This is a topic near and dear to my heart, since it caused me a lot of trouble during my last project. 

Here's a brief summary of the motivation: 

1. We want to sample from distributions like $$\operatorname{Beta}(a, b)$$. However, it's tricky, because up until recently PyTorch could only sample from a few basic distributions (Uniform, Normal, Exponential, etc.). This is a problem because most fast sampling algorithms for more complex distributions work via [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling) (or variants, like ARS), or via [inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling). The first is tricky because if you want to do it in parallel, you need to implement a tricky masking method, and the second is tricky because the inverse CDF is usually a complex function. 

2. Failing that, we can fork out to Numpy! After all, PyTorch seamlessly integrates with Numpy, which has long had excellent support for distributions (more on this later). *However*, sampling in Numpy involves an expensive CPU-GPU copy, which was actually significant, in our models. In our work our baseline used a Beta distribution, so it would be unfair to compare with this large performance hit. 

3. Finally, failing that, we can write C/CUDA code to sample, and link against PyTorch. That's exactly what [we did](https://github.com/rachtsingh/ibp_vae/tree/master/src/lgamma). The downside of this is that CUDA random number generation is a little tricky, and NVIDIA's `cuRAND` library only implements a few random number generators. Also, since I am only a makefile novice, it took me forever to get it to compile on Odyssey, and promptly didn't work when I tried to use it on a different environment. 

So my goal lately is to port some of the knowledge gained to PyTorch proper, so that other researchers can get random $$\operatorname{Beta}(a, b)$$ samples, fast, without having to jump through all the hoops. 

## PyTorch internals

PyTorch as a project is pretty complex, but can be surprisingly easy to contribute to if you know where to look. Unfortunately the documentation on internals is sparse [^1], and there's two things that make it difficult: there's a mixture of C/C++/CUDA/Python code throughout, and it's glued together with *a lot of codegen*. 

Why is this necessary? PyTorch is a Python library that communicates with C/C++ code (for fast CPU operations), and CUDA (for fast GPU operations). Since there are [many data types](http://pytorch.org/docs/master/tensors.html) supported, a lot of the code would be tedious: both
```cpp
THFloatTensor * add(THFloatTensor *a, THFloatTensor *b);
THDoubleTensor * add(THDoubleTensor *a, THDoubleTensor *b);
```
probably have the same implementation. Imagine repeating that 15 times! So not only are the FFI interfaces generated, but the function signatures and implementations too. 

Very recently, [ATen](https://github.com/zdevito/ATen) has made the story somewhat simpler by leveraging C++11 and namespacing. 

Here's a few notes I found useful while trying to understand how the build works: 

1. There are 2 different codegen systems: `cwrap` for generating Python interfaces for some underlying code, and `.yaml` for an interface from `Variable` to `ATen`. So, the `torch/csrc/generic/**/*.cwrap` files generate Python interfaces and versions of the `THTensor_(...)` methods for each type, which are dispatched based on the type used. You can jump into that via `generate_code.py` [here](https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/generate_code.py#L77). 

	For the `.yaml` files, ATen builds its own interface via [this file](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/gen.py) and outputs `Declarations.yaml`. Then, `generate_code.py` reads `Declarations.yaml` and writes the corresponding Python interface, using `gen_variable_type` and the `derivatives.yaml` file. The latter also has information about what the gradient of an operation is. 

2. While building, all the information in [`CONTRIBUTING.md`](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md) is very helpful in keeping iteration time down. Also helpful: rewrite `build_deps` inside `setup.py` to just build your component (e.g. `ATen`). Sometimes it gets screwed up and running `python setup.py clean` is the remedy. 	

Mostly I figured this out by running the build, using `ag -G [something] [term]`, and `find . -name "[regexp]"`. If you're poking around, they will likely be useful as well.

## Some thoughts

One problem that bothered me for more than a week on the IBP project was that our implementation of Beta BBVI went haywire when I used my CUDA sampler. So, following Finale's advice, I made some [qq-plots](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot), but couldn't see any real issues. The reason: was sampling using the identity

$$z \sim \operatorname{Beta}(a, b) \implies z \sim \frac{\operatorname{Gamma}(a)}{\operatorname{Gamma}(a) + \operatorname{Gamma}(b)}$$

since you know, that's what I learned in Stat 210. But! This is numerically unstable when both $$a, b \leq 1$$. The solution was found while digging through Numpy's code [here](https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/distributions.c#L189), which taught me to respect my elders, or at least to respect Numpy. 

I wonder whether there's any work still going on for fast random number sampling. It's not something I'm directly interested in, but something I'm curious about. 

Another fun story: when later trying to calculate log of the Beta function, I was on my guard and checked out [the Cephes implementation](https://github.com/scipy/scipy/blob/master/scipy/special/cephes/beta.c#L138), which is roughly 30 years old. At the top it says: 

> "Direct inquiries to 30 Frost Street, Cambridge, MA 02140"

which is about 2 blocks from where I live.   

[^1]: There's some other blog posts by the PyTorch folks [here](http://pytorch.org/blog/), definitely also worth checking out. 