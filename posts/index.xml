<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Rachit Singh</title>
    <link>https://rachitsingh.com/posts/</link>
    <description>Recent content in Posts on Rachit Singh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jan 2018 12:48:10 -0800</lastBuildDate>
    
	<atom:link href="https://rachitsingh.com/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A few favorite papers of 2017</title>
      <link>https://rachitsingh.com/a-few-favorite-papers-of-2017/</link>
      <pubDate>Tue, 09 Jan 2018 12:48:10 -0800</pubDate>
      
      <guid>https://rachitsingh.com/a-few-favorite-papers-of-2017/</guid>
      <description>This isn&#39;t an exhaustive list, and I will inevitably forget some papers. I&#39;ll keep updating as a remember, and will probably expand some of the background/contribution sections as I have time, so that they&#39;re more accessible.
Breaking the Softmax Bottleneck: A High-Rank RNN Language Model [link] Background: Language models and NLP tasks almost always use a softmax to compute a distribution over the vocabulary, and usually this is computed as \(\sigma(\mathbf{W}\mathbf{h})\), where \(\mathbf{h}\) is a \(d\)-dimensional context vector from a previous layer, and \(\mathbf{W} \in \mathbb{R}^{M \times d}\) is a word embedding, letting $M$ be the vocabulary size.</description>
    </item>
    
    <item>
      <title>PyTorch Internals, cuRAND, and numerical instability</title>
      <link>https://rachitsingh.com/pytorch-internals-curand-and-numerical-instability/</link>
      <pubDate>Wed, 03 Jan 2018 18:44:27 -0800</pubDate>
      
      <guid>https://rachitsingh.com/pytorch-internals-curand-and-numerical-instability/</guid>
      <description>Random sampling I&#39;ve been working lately to implement random samplers from a number of distributions in PyTorch, both on CPU and CUDA. This is a topic near and dear to my heart, since it has caused me a lot of trouble multiple times. Once this PR is merged, I&#39;ll post an explanation/notebook of why this is important.
Here&#39;s a brief summary of the motivation:
 We want to sample from distributions like \(\operatorname{Beta}(a, b)\).</description>
    </item>
    
    <item>
      <title>ELBO Surgery</title>
      <link>https://rachitsingh.com/elbo_surgery/</link>
      <pubDate>Sat, 23 Dec 2017 12:08:00 -0800</pubDate>
      
      <guid>https://rachitsingh.com/elbo_surgery/</guid>
      <description>td { padding: 5px; font-family: monospace; font-size: 1.25rem; } th { text-align: center; padding: 0px 5px; } th.left_column { text-align: right; } figure { margin: 0px 20px; max-width: 50rem; } img[src*=&#34;#smaller&#34;] { width: 50%; margin: auto; }  tldr: The ubiquitous isotropic Gaussian prior for generative models doesn&#39;t make sense / doesn&#39;t work, which motivates work on priors.
At NIPS, Dawen Liang mentioned Hoffman &amp;amp; Johnson&#39;s ELBO surgery paper offhand while talking about tuning KL divergences, and it&#39;s very interesting, so I thought I&#39;d go over it.</description>
    </item>
    
    <item>
      <title>NIPS 2017</title>
      <link>https://rachitsingh.com/nips/</link>
      <pubDate>Sat, 09 Dec 2017 00:47:37 -0800</pubDate>
      
      <guid>https://rachitsingh.com/nips/</guid>
      <description>I&#39;m starting this blog to share research ideas that I have, and some solutions to problems I find along the way. I&#39;ve been helped immensely by other people&#39;s blogs in the past, and want to do the same. Also it&#39;ll give me a chance to communicate the way I approach problems, and hopefully people will give me alternative perspectives either by email (rachitsingh@college.harvard.edu) or in the comments, once I figure out how that works.</description>
    </item>
    
  </channel>
</rss>