<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<title>PyTorch Internals, cuRAND, and numerical instability</title>
	
	<meta name="description" content="">
	<meta name="image" content="">
	
	<meta itemprop="name" content="PyTorch Internals, cuRAND, and numerical instability">
	<meta itemprop="description" content="">
	<meta itemprop="image" content="">
	
	<meta name="og:title" content="PyTorch Internals, cuRAND, and numerical instability">
	<meta name="og:description" content="">
	
	<meta name="og:url" content="https://rachitsingh.com/pytorch-internals-curand-and-numerical-instability/">
	<meta name="og:site_name" content="PyTorch Internals, cuRAND, and numerical instability">
	<meta name="og:type" content="article">
	
	<meta name="article:author" content="Rachit Singh">
	<meta name="article:tag" content="">
	<link rel="stylesheet" type="text/css" href="/css/style.css">
	
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/katex.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/katex.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/contrib/auto-render.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
</head>

<body>



<div id="loadingMask" style="width: 100%; height: 100%; position: fixed; background: #fff;"></div>
<script>
function fadeOut(el) {
  el.style.opacity = 1;

  var last = +new Date();
  var tick = function() {
    el.style.opacity = +el.style.opacity - (new Date() - last) / 80;
    last = +new Date();
    

    if (el.style.opacity > 0) {
      (window.requestAnimationFrame && requestAnimationFrame(tick)) || setTimeout(tick, 16);
    } else {
    	el.style.display='none';
    }
  };

  tick();
}

function ready(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
         el = document.getElementById('loadingMask');
         fadeOut(el);
        var elements = document.querySelectorAll("img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("alt")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("alt"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });

    } else {
        document.addEventListener('DOMContentLoaded', fn);
    }
}
window.onload = ready;
</script>

<div class="content">
	<h1>PyTorch Internals, cuRAND, and numerical instability <aside><a href="/" class="w3-tag">home</a>&nbsp;&nbsp;&nbsp;</aside></h1>
	<h2 id="random-sampling">Random sampling</h2>

<p>I've been working lately to implement random samplers from a number of distributions in PyTorch, both on CPU and CUDA. This is a topic near and dear to my heart, since it has caused me a lot of trouble multiple times. Once <a href="https://github.com/probtorch/pytorch/pull/58">this PR</a> is merged, I'll post an explanation/notebook of why this is important.</p>

<p>Here's a brief summary of the motivation:</p>

<ol>
<li><p>We want to sample from distributions like <span  class="math">\(\operatorname{Beta}(a, b)\)</span>. However, it's tricky, because up until recently PyTorch could only sample from a few basic distributions (Uniform, Normal, Exponential, etc.). This is a problem because most fast sampling algorithms for more complex distributions work via <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> (or variants, like ARS), or via <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a>. The first is tricky because if you want to do it in parallel in pure PyTorch, you need to implement a tricky masking method, and the second is tricky because the inverse CDF is often hard to compute.</p></li>

<li><p>Failing that, we can fork out to Numpy. After all, PyTorch seamlessly integrates with Numpy, which has long had excellent support for distributions (more on this later). However, sampling in Numpy involves an expensive CPU-GPU copy, which was actually significant in our models. In our work, the baseline used a Beta distribution, so it would be unfair to compare with this large performance hit.</p></li>

<li><p>Finally, failing that, we can write C/CUDA code to sample, and link against PyTorch. That's exactly what <a href="https://github.com/rachtsingh/ibp_vae/tree/master/src/lgamma">we did</a>. The downside of this is that CUDA random number generation is a little tricky, and NVIDIA's <code>cuRAND</code> library only implements a few random number generators. Also, since I am only a makefile novice, it took me forever to get it to compile on Odyssey, and promptly didn't work when I tried to use it on a different environment.</p></li>
</ol>

<p>So, my goal lately is to port some of the knowledge gained to PyTorch proper. That way, other researchers can get random <span  class="math">\(\operatorname{Beta}(a, b)\)</span> samples, fast, without having to jump through all the hoops.</p>

<h2 id="pytorch-internals">PyTorch internals</h2>

<p>PyTorch as a project is pretty complex, but can be surprisingly easy to contribute to if you know where to look. Unfortunately the documentation on internals is sparse <sup class="footnote-ref" id="fnref:1"><a class="footnote" href="#fn:1">1</a></sup>, and there's two things that make it difficult: there's a mixture of C/C++/CUDA/Python code throughout, and it's glued together with <em>a lot of codegen</em>.</p>

<p>Why is this necessary? PyTorch is a Python library that communicates with C/C++ code (for fast CPU operations), and CUDA (for fast GPU operations). Since there are <a href="http://pytorch.org/docs/master/tensors.html">many data types</a> supported, a lot of the code would be tedious: all of</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">THFloatTensor * <span style="color:#0a0">add</span>(THFloatTensor *a, THFloatTensor *b);
THDoubleTensor * <span style="color:#0a0">add</span>(THDoubleTensor *a, THDoubleTensor *b);
THCFloatTensor * <span style="color:#0a0">add</span>(THCFloatTensor *a, THCFloatTensor *b);
...</code></pre></div>
<p>probably have the same implementation. Imagine repeating that 15 times! So not only are the FFI interfaces generated, but the function signatures and implementations too.</p>

<p>Very recently, <a href="https://github.com/zdevito/ATen">ATen</a> has made the story somewhat simpler by leveraging C++11 and namespacing to eliminate macros <sup class="footnote-ref" id="fnref:2"><a class="footnote" href="#fn:2">2</a></sup>.</p>

<p>Here's a few notes I found useful while trying to understand how the build works:</p>

<ol>
<li><p>There are 2 different codegen systems: <code>cwrap</code> for generating Python interfaces for some underlying code, and <code>.yaml</code> for an interface from <code>Variable</code> to <code>ATen</code>. So, the <code>torch/csrc/generic/**/*.cwrap</code> files generate Python interfaces and versions of the <code>THTensor_(...)</code> methods for each type, which are dispatched based on the type used. You can jump into that via <code>generate_code.py</code> <a href="https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/generate_code.py#L77">here</a>.</p>

<p>For the <code>.yaml</code> files, ATen builds its own interface via <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/gen.py">this file</a> and outputs <code>Declarations.yaml</code>. Then, <code>generate_code.py</code> reads <code>Declarations.yaml</code> and writes the corresponding Python interface, using <code>gen_variable_type</code> and the <code>derivatives.yaml</code> file. The latter also has information about what the gradient of an operation is.</p></li>

<li><p>While building, all the information in <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md"><code>CONTRIBUTING.md</code></a> is very helpful in keeping iteration time down. Also helpful: rewrite <code>build_deps</code> inside <code>setup.py</code> to just build your component (e.g. <code>ATen</code>). Sometimes it gets screwed up and running <code>python setup.py clean</code> is the remedy.</p></li>

<li><p>The ATen codegen (starting with <code>gen.py</code>, but mostly in <code>function_wrapper.py</code>) generates the glue that dispatches the correct function based on types. After building, you can find these files in <code>torch/lib/build/aten/src/ATen/ATen/</code>. If you want to mess with the generation, you can modify <code>function_wrapper.py</code>: just find the spot where the corresponding code is generated, and modify <code>options</code> to do what you need. Note that to change just one code path, you'll need to modify many of the codegen points, so look for all of them (<code>Functions.h</code>, <code>CPU[Type]Type.h</code>, etc.).</p></li>
</ol>

<p>Mostly I figured this out by running the build, using <code>ag -G [something] [term]</code>, and <code>find . -name &quot;[regexp]&quot;</code>. If you're poking around, they will likely be useful as well. NOTE: by default, <a href="https://github.com/ggreer/the_silver_searcher">ag</a> or <a href="https://github.com/BurntSushi/ripgrep">rg</a> will ignore the files in your <code>.gitignore</code>. This includes generated build files!</p>

<h2 id="a-story-about-rng">A story about RNG</h2>

<p>Recently I was implementing a Poisson sampler using essentially rejection sampling, and found that it didn't work. Here's the code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">__device__ int64_t <span style="color:#0a0">sample_poisson</span>(<span style="color:#0aa">double</span> lambda, curandStateMtgp32 *states) {
  <span style="color:#00a">if</span> (lambda &lt; <span style="color:#099">10</span>) {
    <span style="color:#0aa">double</span> enlam = std::exp(-lambda);
    int64_t X = <span style="color:#099">0</span>;
    <span style="color:#0aa">double</span> prod = <span style="color:#099">1.0</span>;
    <span style="color:#0aa">double</span> U = <span style="color:#099">0</span>;
    <span style="color:#00a">while</span> (<span style="color:#099">1</span>) {
      U = curand_uniform_double(&amp;states[blockIdx.x]);
      prod *= U;
      <span style="color:#00a">if</span> (prod &gt; enlam) {
        X += <span style="color:#099">1</span>;
      }
      <span style="color:#00a">else</span> {
        <span style="color:#00a">return</span> X;
      }
    }
  }
  ... <span style="color:#aaa;font-style:italic">// more special case code for values of lambda
</span><span style="color:#aaa;font-style:italic"></span>}</code></pre></div>
<p>In particular, if a thread didn't exit in the first or second samples, it would never exit the while loop. I spent a while debugging, and realized that even though calls to <code>curand_uniform_double</code> were uniformly distributed in isolation, adding rejection sampling would cause it to repeat values. The calls are <code>curand_uniform_double(state)</code> for some RNG state <code>state</code>, but <code>state</code> was fine since it generated uniform doubles in isolation. PyTorch uses a MTGP32-based sampler, so I eventually looked in the docs and found this line:</p>

<blockquote>
<p>&quot;At a given point in the code, all threads in the block, or none of them, must call this
function.&quot;</p>
</blockquote>

<p>So, what was happening is that threads that returned early didn't call the function, so it was undefined behavior. This means rejection sampling is hard! However, there's a solution. There's an alternative call, <code>curand_mtgp32_single_specific</code>, which takes a generator state, an index, and a count of the total number of threads that call it. As long as each index is unique and adds up the thread count, this will give uniformly distributed floats as expected. However, we do need to be a bit careful about how to synchronize because of <a href="https://cvw.cac.cornell.edu/gpu/thread_div">warp divergence</a>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">__device__ int64_t <span style="color:#0a0">sample_poisson</span>(<span style="color:#0aa">double</span> lambda, curandStateMtgp32 *states, <span style="color:#0aa">int</span> num_threads) {
  __shared__ <span style="color:#0aa">int</span> thread_count;
  <span style="color:#00a">if</span> (threadIdx.x == <span style="color:#099">0</span>) thread_count = num_threads;
  int64_t X = <span style="color:#099">0</span>;
  <span style="color:#0aa">int</span> idx = threadIdx.x;
  <span style="color:#0aa">float</span> U = <span style="color:#099">0</span>;
  <span style="color:#0aa">float</span> enlam = std::exp(-lambda);
  <span style="color:#0aa">float</span> prod = <span style="color:#099">1.0</span>;

  <span style="color:#00a">while</span> (thread_count != <span style="color:#099">0</span>) {
    U = curand_mtgp32_single_specific(&amp;states[blockIdx.x], idx, thread_count);
    prod *= U;
    <span style="color:#00a">if</span> (prod &gt; enlam) {
      X += <span style="color:#099">1</span>;
    }
    __syncthreads();
    <span style="color:#00a">if</span> (idx == <span style="color:#099">0</span>) {
      thread_count = <span style="color:#099">0</span>;
    }
    __syncthreads();
    <span style="color:#00a">if</span> (prod &gt; enlam) {
      idx = atomicAdd(&amp;thread_count, <span style="color:#099">1</span>); <span style="color:#aaa;font-style:italic">// counts &#39;living&#39; threads
</span><span style="color:#aaa;font-style:italic"></span>    }
  }
}</code></pre></div>
<p>While it's neat, for a few reasons unfortunately it's not quite appropriate for PyTorch, so we'll look into other solutions. For the Poisson, at least, there's a <code>curand_poisson</code> which implements it natively for us.</p>

<h2 id="some-thoughts">Some thoughts</h2>

<p>One problem that bothered me for more than a week on the IBP project was that our implementation of Beta BBVI went haywire when I used my CUDA sampler. So, following Finale's advice, I made some <a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">qq-plots</a>, but couldn't see any real issues. The reason: was sampling using the identity</p>

<p><span  class="math">\[z \sim \operatorname{Beta}(a, b) \implies z \sim \frac{\operatorname{Gamma}(a)}{\operatorname{Gamma}(a) + \operatorname{Gamma}(b)}\]</span></p>

<p>since you know, that's what I learned in Stat 210. But! This is numerically unstable when both <span  class="math">\(a, b \leq 1\)</span>. The solution was found while digging through Numpy's code <a href="https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/distributions.c#L189">here</a>, which taught me to respect my elders, or at least to respect Numpy.</p>

<p>I wonder whether there's any work still going on for fast random number sampling. It's not something I'm directly interested in, but something I'm curious about.</p>

<p>Another fun story: when later trying to calculate log of the Beta function, I was on my guard and checked out <a href="https://github.com/scipy/scipy/blob/master/scipy/special/cephes/beta.c#L138">the Cephes implementation</a>, which is roughly 30 years old. At the top it says:</p>

<blockquote>
<p>&quot;Direct inquiries to 30 Frost Street, Cambridge, MA 02140&quot;</p>
</blockquote>

<p>which is about 2 blocks from where I live.</p>
<div class="footnotes">

<hr>

<ol>
<li id="fn:1">There's some other blog posts by the PyTorch folks <a href="http://pytorch.org/blog/">here</a>, definitely also worth checking out.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
<li id="fn:2">Which are the devil. My operating systems course, as excellent as it was, was entirely in C and implemented arrays via macros.
 <a class="footnote-return" href="#fnref:2"><sup>[return]</sup></a></li>
</ol>
</div>

</div>


<footer>

	<p><small><em>Written January 3, 2018. </em>
		 
		Send feedback to <a  href="mailto:rachitsingh@college.harvard.edu">the author</a>.
		
	</small></p>

	<p>
	<a href="/elbo_surgery/">← ELBO Surgery</a>&nbsp;
	
	</p>


</footer>




<script>renderMathInElement(document.body);</script>
</body>
</html>
