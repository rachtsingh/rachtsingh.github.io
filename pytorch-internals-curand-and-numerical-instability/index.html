<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<title>PyTorch Internals, cuRAND, and numerical instability</title>
	
	<meta name="description" content="">
	<meta name="image" content="">
	
	<meta itemprop="name" content="PyTorch Internals, cuRAND, and numerical instability">
	<meta itemprop="description" content="">
	<meta itemprop="image" content="">
	
	<meta name="og:title" content="PyTorch Internals, cuRAND, and numerical instability">
	<meta name="og:description" content="">
	
	<meta name="og:url" content="https://rachitsingh.com/pytorch-internals-curand-and-numerical-instability/">
	<meta name="og:site_name" content="PyTorch Internals, cuRAND, and numerical instability">
	<meta name="og:type" content="article">
	
	<meta name="article:author" content="Rachit Singh">
	<meta name="article:tag" content="">
	<link rel="stylesheet" type="text/css" href="/css/style.css">
	
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/katex.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/katex.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/contrib/auto-render.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
</head>

<body>



<div id="loadingMask" style="width: 100%; height: 100%; position: fixed; background: #fff;"></div>
<script>
function fadeOut(el) {
  el.style.opacity = 1;

  var last = +new Date();
  var tick = function() {
    el.style.opacity = +el.style.opacity - (new Date() - last) / 80;
    last = +new Date();
    

    if (el.style.opacity > 0) {
      (window.requestAnimationFrame && requestAnimationFrame(tick)) || setTimeout(tick, 16);
    } else {
    	el.style.display='none';
    }
  };

  tick();
}

function ready(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
         el = document.getElementById('loadingMask');
         fadeOut(el);
        var elements = document.querySelectorAll("img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("alt")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("alt"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });

    } else {
        document.addEventListener('DOMContentLoaded', fn);
    }
}
window.onload = ready;
</script>

<div class="content">
	<h1>PyTorch Internals, cuRAND, and numerical instability <aside><a href="/" class="w3-tag">home</a>&nbsp;&nbsp;&nbsp;</aside></h1>
	<h2 id="random-sampling">Random sampling</h2>

<p>I've been working lately to implement random samplers from a number of distributions in PyTorch, both on CPU and CUDA. This is a topic near and dear to my heart, since it caused me a lot of trouble during my last project.</p>

<p>Here's a brief summary of the motivation:</p>

<ol>
<li><p>We want to sample from distributions like <span  class="math">\(\operatorname{Beta}(a, b)\)</span>. However, it's tricky, because up until recently PyTorch could only sample from a few basic distributions (Uniform, Normal, Exponential, etc.). This is a problem because most fast sampling algorithms for more complex distributions work via <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> (or variants, like ARS), or via <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a>. The first is tricky because if you want to do it in parallel, you need to implement a tricky masking method, and the second is tricky because the inverse CDF is usually a complex function.</p></li>

<li><p>Failing that, we can fork out to Numpy! After all, PyTorch seamlessly integrates with Numpy, which has long had excellent support for distributions (more on this later). <em>However</em>, sampling in Numpy involves an expensive CPU-GPU copy, which was actually significant, in our models. In our work our baseline used a Beta distribution, so it would be unfair to compare with this large performance hit.</p></li>

<li><p>Finally, failing that, we can write C/CUDA code to sample, and link against PyTorch. That's exactly what <a href="https://github.com/rachtsingh/ibp_vae/tree/master/src/lgamma">we did</a>. The downside of this is that CUDA random number generation is a little tricky, and NVIDIA's <code>cuRAND</code> library only implements a few random number generators. Also, since I am only a makefile novice, it took me forever to get it to compile on Odyssey, and promptly didn't work when I tried to use it on a different environment.</p></li>
</ol>

<p>So my goal lately is to port some of the knowledge gained to PyTorch proper, so that other researchers can get random <span  class="math">\(\operatorname{Beta}(a, b)\)</span> samples, fast, without having to jump through all the hoops.</p>

<h2 id="pytorch-internals">PyTorch internals</h2>

<p>PyTorch as a project is pretty complex, but can be surprisingly easy to contribute to if you know where to look. Unfortunately the documentation on internals is sparse <sup class="footnote-ref" id="fnref:1"><a class="footnote" href="#fn:1">1</a></sup>, and there's two things that make it difficult: there's a mixture of C/C++/CUDA/Python code throughout, and it's glued together with <em>a lot of codegen</em>.</p>

<p>Why is this necessary? PyTorch is a Python library that communicates with C/C++ code (for fast CPU operations), and CUDA (for fast GPU operations). Since there are <a href="http://pytorch.org/docs/master/tensors.html">many data types</a> supported, a lot of the code would be tedious: both</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">THFloatTensor * <span style="color:#0a0">add</span>(THFloatTensor *a, THFloatTensor *b);
THDoubleTensor * <span style="color:#0a0">add</span>(THDoubleTensor *a, THDoubleTensor *b);</code></pre></div>
<p>probably have the same implementation. Imagine repeating that 15 times! So not only are the FFI interfaces generated, but the function signatures and implementations too.</p>

<p>Very recently, <a href="https://github.com/zdevito/ATen">ATen</a> has made the story somewhat simpler by leveraging C++11 and namespacing.</p>

<p>Here's a few notes I found useful while trying to understand how the build works:</p>

<ol>
<li><p>There are 2 different codegen systems: <code>cwrap</code> for generating Python interfaces for some underlying code, and <code>.yaml</code> for an interface from <code>Variable</code> to <code>ATen</code>. So, the <code>torch/csrc/generic/**/*.cwrap</code> files generate Python interfaces and versions of the <code>THTensor_(...)</code> methods for each type, which are dispatched based on the type used. You can jump into that via <code>generate_code.py</code> <a href="https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/generate_code.py#L77">here</a>.</p>

<p>For the <code>.yaml</code> files, ATen builds its own interface via <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/gen.py">this file</a> and outputs <code>Declarations.yaml</code>. Then, <code>generate_code.py</code> reads <code>Declarations.yaml</code> and writes the corresponding Python interface, using <code>gen_variable_type</code> and the <code>derivatives.yaml</code> file. The latter also has information about what the gradient of an operation is.</p></li>

<li><p>While building, all the information in <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md"><code>CONTRIBUTING.md</code></a> is very helpful in keeping iteration time down. Also helpful: rewrite <code>build_deps</code> inside <code>setup.py</code> to just build your component (e.g. <code>ATen</code>). Sometimes it gets screwed up and running <code>python setup.py clean</code> is the remedy.</p></li>
</ol>

<p>Mostly I figured this out by running the build, using <code>ag -G [something] [term]</code>, and <code>find . -name &quot;[regexp]&quot;</code>. If you're poking around, they will likely be useful as well.</p>

<h2 id="some-thoughts">Some thoughts</h2>

<p>One problem that bothered me for more than a week on the IBP project was that our implementation of Beta BBVI went haywire when I used my CUDA sampler. So, following Finale's advice, I made some <a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">qq-plots</a>, but couldn't see any real issues. The reason: was sampling using the identity</p>

<p><span  class="math">\[z \sim \operatorname{Beta}(a, b) \implies z \sim \frac{\operatorname{Gamma}(a)}{\operatorname{Gamma}(a) + \operatorname{Gamma}(b)}\]</span></p>

<p>since you know, that's what I learned in Stat 210. But! This is numerically unstable when both <span  class="math">\(a, b \leq 1\)</span>. The solution was found while digging through Numpy's code <a href="https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/distributions.c#L189">here</a>, which taught me to respect my elders, or at least to respect Numpy.</p>

<p>I wonder whether there's any work still going on for fast random number sampling. It's not something I'm directly interested in, but something I'm curious about.</p>

<p>Another fun story: when later trying to calculate log of the Beta function, I was on my guard and checked out <a href="https://github.com/scipy/scipy/blob/master/scipy/special/cephes/beta.c#L138">the Cephes implementation</a>, which is roughly 30 years old. At the top it says:</p>

<blockquote>
<p>&quot;Direct inquiries to 30 Frost Street, Cambridge, MA 02140&quot;</p>
</blockquote>

<p>which is about 2 blocks from where I live.</p>
<div class="footnotes">

<hr>

<ol>
<li id="fn:1">There's some other blog posts by the PyTorch folks <a href="http://pytorch.org/blog/">here</a>, definitely also worth checking out.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>

</div>


<footer>

	<p><small><em>Written January 3, 2018. </em>
		 
		Send feedback to <a  href="mailto:rachitsingh@college.harvard.edu">the author</a>.
		
	</small></p>

	<p>
	<a href="/elbo_surgery/">‚Üê ELBO Surgery</a>&nbsp;
	
	</p>


</footer>




<script>renderMathInElement(document.body);</script>
</body>
</html>
